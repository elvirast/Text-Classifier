{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification, PreTrainedModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/label_mapping.pickle', 'rb') as f:\n",
    "    map_dct = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasesDataset(Dataset):\n",
    "    def __init__(self, filename, maxlen):\n",
    "        \n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sentence = self.df.loc[index, 'text'] \n",
    "        label = self.df.loc[index, 'label']\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        tokens = ['[CLS]'] + tokens \n",
    "        \n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen] \n",
    "        \n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "        #print('Dataloader:')\n",
    "        #print(tokens_ids_tensor.shape)\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "        \n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self,num_labels=2, freeze_bert = True):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.require_grad = False\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "       \n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.classifier = torch.nn.Linear(768, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        \n",
    "        #_, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        #pooled_output = self.dropout(pooled_output)\n",
    "        #logits = self.classifier(pooled_output)\n",
    "        \n",
    "        cont_reps, _ = self.bert(input_ids, attention_mask = attention_mask)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        x = self.dropout(cls_rep)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CasesDataset('data/train_small.csv', maxlen = 512)\n",
    "test_set = CasesDataset('data/test.csv', maxlen = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "#class_sample_count = train.label.value_counts(sort = False).values.tolist() \n",
    "#weights = 1 / torch.Tensor(class_sample_count)\n",
    "#weights = weights.double()\n",
    "#sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(train_set, batch_size = batch_size, num_workers = 4, shuffle =True)\n",
    "#val_loader = DataLoader(test_set, batch_size = batch_size, num_workers = 4)\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers = 4,\n",
    "                          shuffle = True)\n",
    "val_loader = DataLoader(dataset=test_set, batch_size=batch_size, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMultiLabelSequenceClassification(num_labels = 7, freeze_bert = False)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def evaluate(model, criterion, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "   \n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for it, (seq, attn_masks, labels) in enumerate(dataloader):\n",
    "            #print(seq.size())\n",
    "            #print(attn_masks.size())\n",
    "            #print(labels.size())\n",
    "            seq, attn_masks, labels = seq.cuda(0), attn_masks.cuda(0),labels.cuda(0)\n",
    "            #print(seq.size())\n",
    "            #print(attn_masks.size())\n",
    "            #print(labels.size())\n",
    "            logits = model(seq, attention_mask = attn_masks)\n",
    "            #print(logits.size())\n",
    "            #print(labels.size())\n",
    "            mean_loss += criterion(logits, labels).item()\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count\n",
    "\n",
    "def evaluate_v2(model, test_loader, map_dct):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for it, (seq, attn_masks, labels) in enumerate(test_loader):\n",
    "\n",
    "                seq, attn_masks, labels = seq.cuda(0), attn_masks.cuda(0),labels.cuda(0)          \n",
    "                logits = model(seq, attention_mask = attn_masks)\n",
    "                \n",
    "                y_pred.extend(torch.argmax(logits, 1).tolist())\n",
    "                y_true.extend(labels.tolist())\n",
    "                if it%100==0:\n",
    "                    print(it, ' iterations complete')\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, digits=4, target_names = map_dct.keys()))\n",
    "    return y_true, y_pred\n",
    "    #cm = confusion_matrix(y_true, y_pred)\n",
    "    #ax= plt.subplot()\n",
    "    #sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "    #ax.set_title('Confusion Matrix')\n",
    "\n",
    "    #ax.set_xlabel('Predicted Labels')\n",
    "    #ax.set_ylabel('True Labels')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26836"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 of epoch 1 complete. Loss : 1.6598114967346191 \n",
      "Iteration 20 of epoch 1 complete. Loss : 1.5736589431762695 \n",
      "Iteration 30 of epoch 1 complete. Loss : 1.7666893005371094 \n",
      "Iteration 40 of epoch 1 complete. Loss : 1.6009200811386108 \n",
      "Iteration 50 of epoch 1 complete. Loss : 1.5415629148483276 \n",
      "Iteration 60 of epoch 1 complete. Loss : 1.290280818939209 \n",
      "Iteration 70 of epoch 1 complete. Loss : 1.2392823696136475 \n",
      "Iteration 80 of epoch 1 complete. Loss : 1.0721619129180908 \n",
      "Iteration 90 of epoch 1 complete. Loss : 0.8319083452224731 \n",
      "Iteration 100 of epoch 1 complete. Loss : 1.1615474224090576 \n",
      "Iteration 110 of epoch 1 complete. Loss : 1.1091748476028442 \n",
      "Iteration 120 of epoch 1 complete. Loss : 0.6013498306274414 \n",
      "Iteration 130 of epoch 1 complete. Loss : 0.8384524583816528 \n",
      "Iteration 140 of epoch 1 complete. Loss : 1.11197829246521 \n",
      "Iteration 150 of epoch 1 complete. Loss : 0.901842474937439 \n",
      "Iteration 160 of epoch 1 complete. Loss : 0.8336580991744995 \n",
      "Iteration 170 of epoch 1 complete. Loss : 1.159576177597046 \n",
      "Iteration 180 of epoch 1 complete. Loss : 0.6094784140586853 \n",
      "Iteration 190 of epoch 1 complete. Loss : 0.8878173828125 \n",
      "Iteration 200 of epoch 1 complete. Loss : 0.5806050300598145 \n",
      "Iteration 210 of epoch 1 complete. Loss : 0.6404825448989868 \n",
      "Iteration 220 of epoch 1 complete. Loss : 1.203134298324585 \n",
      "Iteration 230 of epoch 1 complete. Loss : 0.5454162955284119 \n",
      "Iteration 240 of epoch 1 complete. Loss : 0.9808045625686646 \n",
      "====================================================================================================\n",
      "Epoch 1 complete!  Validation Loss : 0.6615991740530808\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    bank_service     0.7084    0.7698    0.7378      2007\n",
      "     credit_card     0.8255    0.6261    0.7121      2955\n",
      "credit_reporting     0.7698    0.8920    0.8264      8123\n",
      " debt_collection     0.8130    0.7761    0.7941      6146\n",
      "            loan     0.6407    0.7858    0.7058      3104\n",
      " money_transfers     0.9603    0.3066    0.4647       473\n",
      "        mortgage     0.9430    0.7435    0.8315      4028\n",
      "\n",
      "        accuracy                         0.7822     26836\n",
      "       macro avg     0.8087    0.7000    0.7246     26836\n",
      "    weighted avg     0.7957    0.7822    0.7802     26836\n",
      "\n",
      "Iteration 10 of epoch 2 complete. Loss : 0.3224332630634308 \n",
      "Iteration 20 of epoch 2 complete. Loss : 0.5737212896347046 \n",
      "Iteration 30 of epoch 2 complete. Loss : 0.41253477334976196 \n",
      "Iteration 40 of epoch 2 complete. Loss : 1.0863752365112305 \n",
      "Iteration 50 of epoch 2 complete. Loss : 0.899263858795166 \n",
      "Iteration 60 of epoch 2 complete. Loss : 0.3340604603290558 \n",
      "Iteration 70 of epoch 2 complete. Loss : 0.2514021098613739 \n",
      "Iteration 80 of epoch 2 complete. Loss : 0.6516009569168091 \n",
      "Iteration 90 of epoch 2 complete. Loss : 0.3764615058898926 \n",
      "Iteration 100 of epoch 2 complete. Loss : 0.4949917793273926 \n",
      "Iteration 110 of epoch 2 complete. Loss : 0.28358983993530273 \n",
      "Iteration 120 of epoch 2 complete. Loss : 0.6025176048278809 \n",
      "Iteration 130 of epoch 2 complete. Loss : 0.5701476335525513 \n",
      "Iteration 140 of epoch 2 complete. Loss : 0.594840407371521 \n",
      "Iteration 150 of epoch 2 complete. Loss : 0.4387393593788147 \n",
      "Iteration 160 of epoch 2 complete. Loss : 0.52554851770401 \n",
      "Iteration 170 of epoch 2 complete. Loss : 0.6611526012420654 \n",
      "Iteration 180 of epoch 2 complete. Loss : 0.302695631980896 \n",
      "Iteration 190 of epoch 2 complete. Loss : 0.6663419008255005 \n",
      "Iteration 200 of epoch 2 complete. Loss : 0.5632845163345337 \n",
      "Iteration 210 of epoch 2 complete. Loss : 0.535495936870575 \n",
      "Iteration 220 of epoch 2 complete. Loss : 0.5718222260475159 \n",
      "Iteration 230 of epoch 2 complete. Loss : 0.5028601884841919 \n",
      "Iteration 240 of epoch 2 complete. Loss : 0.3201161026954651 \n",
      "====================================================================================================\n",
      "Epoch 2 complete!  Validation Loss : 0.5514863399928602\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    bank_service     0.7679    0.7583    0.7631      2007\n",
      "     credit_card     0.7443    0.7939    0.7683      2955\n",
      "credit_reporting     0.8431    0.8571    0.8500      8123\n",
      " debt_collection     0.8348    0.7895    0.8115      6146\n",
      "            loan     0.7328    0.8109    0.7698      3104\n",
      " money_transfers     0.9404    0.4334    0.5933       473\n",
      "        mortgage     0.8980    0.8870    0.8925      4028\n",
      "\n",
      "        accuracy                         0.8189     26836\n",
      "       macro avg     0.8230    0.7614    0.7784     26836\n",
      "    weighted avg     0.8219    0.8189    0.8183     26836\n",
      "\n",
      "Iteration 10 of epoch 3 complete. Loss : 0.21405768394470215 \n",
      "Iteration 20 of epoch 3 complete. Loss : 0.14312419295310974 \n",
      "Iteration 30 of epoch 3 complete. Loss : 0.03652629256248474 \n",
      "Iteration 40 of epoch 3 complete. Loss : 0.042102620005607605 \n",
      "Iteration 50 of epoch 3 complete. Loss : 0.2919439971446991 \n",
      "Iteration 60 of epoch 3 complete. Loss : 0.2513631582260132 \n",
      "Iteration 70 of epoch 3 complete. Loss : 0.20880350470542908 \n",
      "Iteration 80 of epoch 3 complete. Loss : 0.11009310185909271 \n",
      "Iteration 90 of epoch 3 complete. Loss : 0.6242400407791138 \n",
      "Iteration 100 of epoch 3 complete. Loss : 0.13393032550811768 \n",
      "Iteration 110 of epoch 3 complete. Loss : 0.35722506046295166 \n",
      "Iteration 120 of epoch 3 complete. Loss : 0.2974381148815155 \n",
      "Iteration 130 of epoch 3 complete. Loss : 0.07746769487857819 \n",
      "Iteration 140 of epoch 3 complete. Loss : 0.12657909095287323 \n",
      "Iteration 150 of epoch 3 complete. Loss : 0.04859435558319092 \n",
      "Iteration 160 of epoch 3 complete. Loss : 0.2413111925125122 \n",
      "Iteration 170 of epoch 3 complete. Loss : 0.20662838220596313 \n",
      "Iteration 180 of epoch 3 complete. Loss : 0.07643783092498779 \n",
      "Iteration 190 of epoch 3 complete. Loss : 0.08713865280151367 \n",
      "Iteration 200 of epoch 3 complete. Loss : 0.2950409948825836 \n",
      "Iteration 210 of epoch 3 complete. Loss : 0.11997000873088837 \n",
      "Iteration 220 of epoch 3 complete. Loss : 0.20724423229694366 \n",
      "Iteration 230 of epoch 3 complete. Loss : 0.16021586954593658 \n",
      "Iteration 240 of epoch 3 complete. Loss : 0.18219171464443207 \n",
      "====================================================================================================\n",
      "Epoch 3 complete!  Validation Loss : 0.6135310297953998\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    bank_service     0.7256    0.7788    0.7513      2007\n",
      "     credit_card     0.8088    0.7086    0.7554      2955\n",
      "credit_reporting     0.8268    0.8657    0.8458      8123\n",
      " debt_collection     0.8423    0.7511    0.7941      6146\n",
      "            loan     0.7538    0.7674    0.7605      3104\n",
      " money_transfers     0.8645    0.4989    0.6327       473\n",
      "        mortgage     0.8150    0.9459    0.8756      4028\n",
      "\n",
      "        accuracy                         0.8098     26836\n",
      "       macro avg     0.8053    0.7595    0.7736     26836\n",
      "    weighted avg     0.8113    0.8098    0.8078     26836\n",
      "\n",
      "Iteration 10 of epoch 4 complete. Loss : 0.049704164266586304 \n",
      "Iteration 20 of epoch 4 complete. Loss : 0.05530485510826111 \n",
      "Iteration 30 of epoch 4 complete. Loss : 0.02944287657737732 \n",
      "Iteration 40 of epoch 4 complete. Loss : 0.030259251594543457 \n",
      "Iteration 50 of epoch 4 complete. Loss : 0.06787572801113129 \n",
      "Iteration 60 of epoch 4 complete. Loss : 0.028081059455871582 \n",
      "Iteration 70 of epoch 4 complete. Loss : 0.014116615056991577 \n",
      "Iteration 80 of epoch 4 complete. Loss : 0.028485268354415894 \n",
      "Iteration 90 of epoch 4 complete. Loss : 0.07990555465221405 \n",
      "Iteration 100 of epoch 4 complete. Loss : 0.03600029647350311 \n",
      "Iteration 110 of epoch 4 complete. Loss : 0.09897898137569427 \n",
      "Iteration 120 of epoch 4 complete. Loss : 0.09072816371917725 \n",
      "Iteration 130 of epoch 4 complete. Loss : 0.12860988080501556 \n",
      "Iteration 140 of epoch 4 complete. Loss : 0.15851715207099915 \n",
      "Iteration 150 of epoch 4 complete. Loss : 0.07722020149230957 \n",
      "Iteration 160 of epoch 4 complete. Loss : 0.051546260714530945 \n",
      "Iteration 170 of epoch 4 complete. Loss : 0.06381396949291229 \n",
      "Iteration 180 of epoch 4 complete. Loss : 0.02226543426513672 \n",
      "Iteration 190 of epoch 4 complete. Loss : 0.026113241910934448 \n",
      "Iteration 200 of epoch 4 complete. Loss : 0.04715929925441742 \n",
      "Iteration 210 of epoch 4 complete. Loss : 0.023696765303611755 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 220 of epoch 4 complete. Loss : 0.01659846305847168 \n",
      "Iteration 230 of epoch 4 complete. Loss : 0.07759702205657959 \n",
      "Iteration 240 of epoch 4 complete. Loss : 0.12310564517974854 \n",
      "====================================================================================================\n",
      "Epoch 4 complete!  Validation Loss : 0.695194383690198\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    bank_service     0.7512    0.7733    0.7621      2007\n",
      "     credit_card     0.6627    0.8190    0.7326      2955\n",
      "credit_reporting     0.8720    0.8051    0.8372      8123\n",
      " debt_collection     0.8383    0.7691    0.8022      6146\n",
      "            loan     0.7022    0.8289    0.7603      3104\n",
      " money_transfers     0.7108    0.6237    0.6644       473\n",
      "        mortgage     0.9190    0.8898    0.9041      4028\n",
      "\n",
      "        accuracy                         0.8083     26836\n",
      "       macro avg     0.7795    0.7870    0.7804     26836\n",
      "    weighted avg     0.8168    0.8083    0.8102     26836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "num_labels = 7\n",
    "for epoch in range(num_epochs):\n",
    "    for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        seq, attn_masks, labels = seq.cuda(0), attn_masks.cuda(0), labels.cuda(0)\n",
    "        logits = model(seq, attention_mask = attn_masks)\n",
    "        #print(labels.size())\n",
    "        #print(logits.size())\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (it + 1) % 10 == 0 :\n",
    "            print(\"Iteration {} of epoch {} complete. Loss : {} \".format(it+1, epoch+1, loss.item()))\n",
    "    print('='*100)\n",
    "    \n",
    "    val_loss = evaluate(model, criterion, val_loader)\n",
    "    print(\"Epoch {} complete!  Validation Loss : {}\".format(epoch+1, val_loss))\n",
    "    evaluate_v2(model, val_loader, map_dct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
